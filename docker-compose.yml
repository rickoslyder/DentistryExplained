version: '3.8'

services:
  # GPT-Researcher Python Service
  gpt-researcher:
    build:
      context: ./python-research-service
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - FAST_LLM_MODEL=${FAST_LLM_MODEL:-gpt-4o-mini}
      - SMART_LLM_MODEL=${SMART_LLM_MODEL:-gpt-4o}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-small}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - RESEARCH_SERVICE_AUTH_TOKEN=${RESEARCH_SERVICE_AUTH_TOKEN}
      - FRONTEND_URL=${FRONTEND_URL:-http://localhost:3000}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Next.js Application (optional - for full stack deployment)
  # Uncomment if you want to run both services together
  # nextjs:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile
  #   ports:
  #     - "3000:3000"
  #   environment:
  #     - NODE_ENV=production
  #     - RESEARCH_SERVICE_URL=http://gpt-researcher:8000
  #     - RESEARCH_SERVICE_AUTH_TOKEN=${RESEARCH_SERVICE_AUTH_TOKEN}
  #     # Add other Next.js environment variables here
  #   depends_on:
  #     gpt-researcher:
  #       condition: service_healthy
  #   restart: unless-stopped

# Optional: Add a reverse proxy for production
# nginx:
#   image: nginx:alpine
#   ports:
#     - "80:80"
#     - "443:443"
#   volumes:
#     - ./nginx.conf:/etc/nginx/nginx.conf
#     - ./ssl:/etc/nginx/ssl
#   depends_on:
#     - gpt-researcher
#   restart: unless-stopped